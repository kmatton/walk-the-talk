# Project Data

The directories ``bbq-example`` and ``medqa-example`` contain the outputs from the example notebooks in the ``notebooks`` directory of this repo. 

The ``iclr-2025`` directory contains the data (i.e., LLM outputs) used in our ICLR 2025 paper. It contains two sub-directories, ``bbq`` and ``medqa``, which contain the outputs for our experiments on the BBQ and MedQA datasets, respectively. There are three sub-directories within both the ``bbq`` and ``medqa`` folders. They are as follows:

## Counterfactual Generation

The ``counterfactual-generation`` directory contains the LLM outputs involved in generating counterfactual questions. These outputs were generated by the auxiliary LLM (in this case GPT-4o). There is a sub-directory named ``example_*`` for each question used in our analysis. In each of these directories, there are the following files:
* ``concepts.json``: The concepts identified for the question. This is a list of concepts.
* ``concept_settings.json``: The concept values identified for the question. This is a list of dictionaries, where there is a dictionary of values for each concept in ``concepts.json``. In each directory, the ``current_setting`` entry corresponds to the value of the concept in the original question and the ``new_settings`` entry contains a list of alternative values (note: in our BBQ analysis we extracted a single alternative value for each concept).
* ``counterfactual_*.json``: a dictionary with information corresponding to the counterfactual question. The files are named so that there is a `0` for each concept that was not changed, there is a `-` for each concept that was removed, and there is `1` for each concept that was replaced with its alternative value. The counterfactual question is in the ``counterfactual`` entry of the dictionary. The different parts of the counterfacutal question (i.e., context, question, and answer choices) are in the ``parsed_counterfactual`` entry.

## Model Responses

The ``model-responses`` directory contains LLM responses to both the original and counterfactual questions. It contains a sub-directory for each LLM that we collected responses for. For example, ``claude-few-shot-cot`` contains responses from Claude where the question was asked using a few-shot chain-of-thought prompt. Within each LLM-specific directory, there is a sub-directory named ``example_*`` for each question used in our analysis. Within each example directory, there are two sub-directories: ``original`` contains the response to the original question and ``counterfactual`` contains the responses to the counterfactual questions. There are 50 responses for each question (to account for stochasticity in LLM responses). The original responses are named ``response_n=i.json`` where ``i`` is the index of the response (0-49). The counterfactual responses are named ``response_counterfactual=*_n=i.json``, where ``*`` corresponds to the counterfactual name (we use the same naming scheme as the ``counterfactual_*.json`` files mentioned above) and ``i`` is the response index. Each file contains a dictonary with three entries: (1) ``prompt``: the prompt we used to ask the question, (2) ``response``: the LLM's response, and (3) ``answer``: the extracted multiple choice answer selected (parsed from the ``response``).

## Implied Concepts

The ``implied-concepts`` directory contains analyses of the LLM responses (from Model Responses, above) where we determine which concepts the LLM's explanations implied influenced the LLM's answer. These analyses were generated by the auxiliary LLM (in this case GPT-4o). Like the ``model-responses`` directory, this directory contains a sub-directory for each LLM that we collected responses for. Within each LLM-specific directory, there is a sub-directory named ``example_*`` for each question used in our analysis. Within each example directory there is a single directory named ``original`` (as described in Appendix C.1 in our paper, we only perform this "implied concepts" analysis for LLM responses to original questions). We analyze the implied concepts for each of the 50 LLM responses (as described above) to each question. The files are named ``implied_concepts_response_n=i.json``, where ``i`` is the response index. Each file contains a dictionary with three entries: (1) ``prompt``: the prompt we use to extract the implied concepts set, (2) ``responses``: a list of LLM responses to the prompt (note: we only collect a single one), and (3) ``concept_decisions``: for each concept (in order), a binary decision (0 or 1) as to whether the LLM explanation implies that the concept influenced the LLM's decision.

We note that there are three missing files for our analysis of Claude's explanations on the MedQA dataset: one for question 161 and two for question 5. This is because the resulting LLM outputs from this step were invalid  (see the ``failed_examples.json`` file in the ``claude-few-shot-cot`` directory for details). When computing explanation implied effects, we dropped these three examples from our analysis (i.e., computed empirical probabilities without them).
